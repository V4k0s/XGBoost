{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XGBOOST-V2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# XGBoost\n"
      ],
      "metadata": {
        "id": "731SurN-p1Ut"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introdução\n",
        "\n",
        "<p align = \"justify\">O XGboost (Extreme Gradient Boosting) é um algoritmo de machine learning que vêm mostrando excelentes resultados, sobretudo em competições do Kagggle. Tal algoritmo é de tipo Ensemble, podendo ser pensado como uma aprimoração do Gradient Boosting. </p>\n",
        "\n",
        "<p align = \"justify\"> O Gradient Boosting ajusta os pesos usando o Gradient Descent (em geral olhando para uma loss function específica: MSE para algoritmos de regressão e uma função logarítmica para algoritmos de classificação), otimizando iterativamente a diferença entre o valor previsto e o valor real. Porém, o Gradient Boosting é sequencial, não podendo ser usado em computação paralela e distribuída. Isso motiva a construção do XGBoost que será apresentada ao longo desse material. </p>\n",
        "\n",
        "<p align = \"justify\"> Em essência, o XGBoost é um algoritmo que usa a expansão de Taylor de ordem 2 para tratar a função objetivo, tomando árvores de decisão como seus preditores. Além disso, a função objetivo do XGBoost conta com um parâmetro extra de regularização em sua construção, o qual aumenta a precisão, penaliza modelos muito complexos em detrimento de modelos mais simples e evita o temido sobreajuste (ideia que lembra muito a Ridge Regression).</p>\n",
        "\n",
        "<p align = \"justify\"> O que faz do XGBoost tão importante não é apenas os seus excelentes resultados mas, tão importante quanto, é a escalabilidade do modelo, uma vez que permite computação paralela e distribuida, permitindo inclusive a construção de modelos out-of-core: o sistema é executado mais de dez vezes mais rápido do que as soluções populares existentes em uma única máquina e pode ser dimensionado para bilhões de exemplos em configurações distribuídas ou com limitação de memória.</p>\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZtXjBY_7i3QT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fundamentação Teórica"
      ],
      "metadata": {
        "id": "ClpBfwDwqfsZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Função objetivo regularizada\n",
        "\n",
        "\n",
        "<p align = \"justify\"> Seja $\\mathcal{D}=\\{ (x_i,y_i)\\}_{i=1}^{n}⊂\\mathbb{R}^m \\times \\mathbb{R}$ uma amostra rotulada. Um modelo emsemble de árvore de decisão usa um número $K$ de funções aditivas $f_k$ para predizer o parâmetro $y_i$. Essa predição é denotada por $p_i$ e </p>\n",
        "\n",
        "$$p_i = \\sum_{k=1}^{K} f_k(x_i),\\qquad  f_k ∈ \\mathcal{F},$$\n",
        "\n",
        "em que $\\mathcal{F} = \\{f \\mid f(x) = w_{q(x)},\\, q:\\mathbb{R}^m\\to\n",
        "\\{ 1, \\cdots, T\\} \\text{ e } w \\in \\mathbb{R}^T\\}.$\n",
        "\n",
        "<p align = \"justify\"> Cada $f_k$ corresponde a uma estrutura de árvore independente $q$ e pesos de folha $w$, com $T$ sendo o número de folhas da árvore.<b> Ao contrário das árvores de decisão, cada árvore de regressão contém uma pontuação contínua em cada folha: </b> os $w_i$'s são usados para representar a pontuação na $i$-ésima folha. Para cada $x_i$, usaremos as regras de decisão nas árvores (dadas por $q$) para classificá-lo nas folhas e calcular a previsão final somando a pontuação nas folhas correspondentes (dadas por $w$). Para aprender o conjunto de funções usadas no modelo, se minimiza o objetivo regularizado: </p>\n",
        "\n",
        "$$\\mathcal{L} =  \\sum_{i} l(y_i,p_i) +\\sum_{k}\\Omega(f_k)$$\n",
        "onde $$\\Omega(f) = \\Omega(w_q(x) ) = \\gamma T + \\frac{1}{2}\\lambda \\|w\\|^2.$$ \n",
        "\n",
        "<p align = \"justify\">\n",
        "Aqui $l$ é uma \"loss function\" (convexa e diferenciável) que mede a diferença entre a previsão $p_i$ e o alvo $y_i$. O segundo termo $\\Omega$ penaliza a complexidade do modelo (ou seja, as funções da árvore de regressão). Esse termo de regularização ajuda a suavizar os pesos finais aprendidos para evitar sobreajuste (mesma ideia utilizada na construção do ridge regression - para mais detalhes sobre esse algoritmo, veja o curso na udemy: machine learning, do Prof. Edson Cilos). Intuitivamente, o objetivo regularizado tenderá a selecionar um modelo empregando funções simples e preditivas. Quando $\\Omega = 0$, o problema acima coincide com o objetivo do GBoost.\n"
      ],
      "metadata": {
        "id": "utiOAuC0GSDN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### O algoritmo XGBoost\n",
        "\n",
        "\n",
        "<p align = \"justify\"> Seja $p_i^{(t)}$ a previsão da $i$-ésima instância na $t$-ésima iteração. Por construção, fazemos $p_i^{(t)}(x_i) = p_i^{(t - 1)}(x_i) + f_{t}(x_i)$, portanto, acrescentamos na iteração $t$, para cada $x_i$, \n",
        "o termo $f_{t}(x_i)$. Na prática queremos encontrar  $f_t$ afim de minimizar o problema\n",
        "\n",
        "$$\\mathcal{L}^{(t)} =  \\sum_{i= 1}^{n} l(y_i,p_i^{(t-1)}+f_t(x_i)) +\\Omega(f_t)$$\n",
        "\n",
        "\n",
        "<p align = \"justify\">O que é equivalente a encontrar o peso $w_{q(x)} = f(x)$ para cada uma das folhas, isto é, encontrar cada um dos $w_i$'s que é o peso associado à $i$-ésima folha, com $i \\in \\{1, \\cdots, T \\}.$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Usando a expansão de Taylor de ordem 2, sabemos que \n",
        "\n",
        "$$f(a+h)≈ f(a)+f'(a)h+\\frac{1}{2}f''(a)h^2$$ para $f\\in C^2$. Se $l$ for suficientemente suave, fazemos $f=l$, $a = p_i^{(t)}$ e $h=f_t(x_i)$, obtendo assim\n",
        "\n",
        "\\begin{align}\n",
        "\\mathcal{L}^{(t)} &≈  \\sum_{i= 1}^{n} \\bigl( l(y_i,p_i^{(t-1)})+g_if_t(x_i)+\\frac{1}{2}h_i [f_t(x_i)]^2\\bigr) +\\Omega(f_t) \\\\ &= {C} + \\sum_{i= 1}^{n} \\bigl[ g_if_t(x_i)+\\frac{1}{2}h_i [f_t(x_i)]^2\\bigr) +\\Omega(f_t)\n",
        "\\end{align}\n",
        "em que $C = \\sum_{i}^{n} l(y_i,p_i^{(t-1)}) $ é uma constante uma vez que essa expressão já é conhecido na iteração $t - 1$. Além disso: $$g_i = \\frac{\\partial l(y_i, p_i^{(t-1)})}{\\partial p_i^{(t-1)}} \\qquad \\text{ e } \\qquad h_i = \\frac{\\partial^2 l(y_i, p_i^{(t-1)})}{\\partial p_i^{(t-1)}\\,^2 }.$$\n",
        "\n",
        "\n",
        "<p align = \"justify\"> Removendo o termo constante $C$, nossa nova função objetivo se torna no passo $t$ igual a \n",
        "\n",
        "$$\\mathcal{L_1}(t) = \\Omega(f_t)+ \\sum_{i= 1}^{n} \\bigl(g_if_t(x_i)+\\frac{1}{2}h_i [f_t(x_i)]^2\\bigr).$$\n",
        "\n",
        "\n",
        "<p align = \"justify\">Dado $j\\in\\{1,\\dots, T\\}$, defina $I_j  = \\{i\\mid q(x_i)=j\\}$. Expandindo $\\Omega$ e retomando definição de $\\mathcal{F}$, podemos escrever\n",
        "\\begin{align}\n",
        "\\mathcal{L}_1^{(t)} &=\\gamma T + \\frac{1}{2}\\lambda \\sum_{j=1}^{T} w_j^2+\\sum_{i= 1}^{n} g_if_t(x_i)+\\frac{1}{2}h_i [f_t(x_i)]^2 \\\\& = \\gamma T + \\frac{1}{2}\\lambda \\sum_{j=1}^{T} w_j^2+ \\sum_{j= 1}^{T}\\sum_{i \\in I_j} g_iw_j+\\frac{1}{2}\\sum_{j= 1}^{T}\\sum_{i \\in I_j} h_i[w_j]^2\\\\& = \\gamma T + \\sum_{j=1}^{T} \\biggl[ \\sum_{i \\in I_j} g_iw_j+ \\frac{1}{2}w_j^2\\biggl( \\lambda + \\sum_{i \\in I_j} h_i \\biggr)\\biggr]\n",
        "\\end{align}\n",
        "\n",
        "<p align = \"justify\"> <b> Uma vez fixada a estrutura $\\boldsymbol{q(x)}$</b>, podemos calcular para cada $j$ o peso ótimo $w_j^*$. Fixe $j \\in \\{1,\\dots,T \\}$. A ideia é relativamente simples: para minimizarmos a soma, basta minimizarmos cada um de seus fatores. Portanto, queremos minimizar\n",
        "\\begin{equation*}\n",
        "R(w_j) = \\sum_{i \\in I_j} g_iw_j+ \\frac{1}{2}w_j^2\\biggl( \\lambda + \\sum_{i \\in I_j} h_i \\biggr).\n",
        "\\end{equation*}\n",
        "<p align = \"justify\"> Sendo $l$ uma função convexa, segue que $h_i\\geq 0$ para qualquer $i\\in I_j$. Assim, o mínimo da parábola $R(w_j)$ é dado no ponto $w_j^*$ tal que $R'(w_j^*)=0$. Logo\n",
        "\\begin{equation*}\n",
        "0 = R'(w_j^*) = \\sum_{i \\in I_j} g_i+w_j^*\\biggl( \\lambda + \\sum_{i \\in I_j} h_i \\biggr),\n",
        "\\end{equation*}\n",
        "ou seja, o peso $w_j^*$ ótimo é dado por\n",
        "$$\n",
        "w_j^* = -\\frac{\\sum_{i \\in I_j} g_i}{\\lambda + \\sum_{i \\in I_j} h_i}\n",
        "$$\n",
        "<p align = \"justify\">Por conseguinte, o valor ótimo correspondente é </p>\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_1^{(t)}(q) = \\gamma T-\\frac{1}{2}\\sum_{j=1}^{T}\\frac{\\bigl(\\sum_{i \\in I_j} g_i\\bigr)^2}{\\lambda + \\sum_{i \\in I_j} h_i},\n",
        "$$\n",
        "\n",
        "\n",
        "<p align = \"justify\">para cada estrutura de árvore fixa $q$. Adicionalmente, para cada estrutura de árvore $q$ você pode usar a expressão encontrada para $\\mathcal{L}_1^{(t)}(q)$ como uma medida da \"perda\" da árvore $q$, como se medisse o grau de impureza.\n",
        "\n",
        "<p align = \"justify\"> Todavia, de modo geral, não é possível enumerar todas as estruturas de árvores $q$ à fim de encontrar aquela estrutura que forneça a menor impureza </font>. Em vez disso, usa-se um algoritmo ambicioso que começa a partir de uma única folha e adiciona iterativamente ramos à árvore. Se $I_L$ e $I_R$ são os conjuntos de instâncias dos nós esquerdo e direito após a divisão, então deixando $I = I_L \\cup I_R$ a redução da perda após a divisão é dada por</p>\n",
        "\n",
        "\\begin{align}\n",
        "\\mathcal{L}_{split} &= \\gamma -\\frac{1}{2}\\frac{\\bigl(\\sum_{i \\in I} g_i\\bigr)^2}{\\lambda + \\sum_{i \\in I} h_i} - \\biggl[\\gamma-\\frac{1}{2}\\frac{\\bigl(\\sum_{i \\in I_L} g_i\\bigr)^2}{\\lambda + \\sum_{i \\in I_L} h_i} + \\gamma -\\frac{1}{2}\\frac{\\bigl(\\sum_{i \\in I_R} g_i\\bigr)^2}{\\lambda + \\sum_{i \\in I_R} h_i} \\biggr]\\\\&=\\frac{1}{2}\\biggl[ \\frac{\\bigl(\\sum_{i \\in I_L} g_i\\bigr)^2}{\\lambda + \\sum_{i \\in I_L} h_i} +\\frac{\\bigl(\\sum_{i \\in I_R} g_i\\bigr)^2}{\\lambda + \\sum_{i \\in I_R} h_i} -\\frac{\\bigl(\\sum_{i \\in I} g_i\\bigr)^2}{\\lambda + \\sum_{i \\in I} h_i}\\biggr]-\\gamma.\n",
        "\\end{align}\n"
      ],
      "metadata": {
        "id": "Z9RV943Ppl8f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pseudocódigo do XGBoost - versão em série\n",
        "\n",
        "\n",
        "<p align = \"justify\"> Input: conjunto de treinamento $\\{(x_{i},y_{i})\\}_{i=1}^{n}$, uma função de perda diferenciável $l(y, F(x))$ de classe $C^2$, um número $M$ de modelos de predição mais simples (árvores de decisão, por exemplo) e uma taxa de aprendizagem $\\alpha$.\n",
        "\n",
        "Algoritmo:\n",
        "\n",
        "1. Inicializar o modelo com um valor constante:\n",
        "\n",
        "$${p^{(0)}={\\underset {\\theta }{\\arg \\min }}\\sum _{i=1}^{n}l(y_{i},\\theta ).}$$\n",
        "\n",
        "2. Para $m = 1$ até $M$:\n",
        "    1. Computar os \"gradientes\" e as \"hessianas\":\n",
        "\\begin{align}\n",
        "{\\hat {g}}_{m}(x_{i})&=\\left[{\\frac {\\partial l(y_{i},f(x_{i}))}{\\partial f(x_{i})}}\\right]\\biggl|_{p^{(m-1)}}\\\\\n",
        "{\\hat {h}}_{m}(x_{i})&=\\left[{\\frac {\\partial ^{2}l(y_{i},f(x_{i}))}{\\partial f(x_{i})^{2}}}\\right]\\biggl|_{p^{(m-1)}}.\n",
        "\\end{align}\n",
        "    2. Ajustar a base de aprendizado usando o conjunto de treinamento\n",
        "$\\left\\{x_{i},-\\frac{\\hat g_m (x_{i})}{ \\hat {h}_{m}(x_{i})}\\right\\}_{i=1}^{n}$ resolvendo o problema de otimização:\n",
        "$$\n",
        "{\\displaystyle {\\hat {\\phi }}_{m}={\\underset {\\phi \\in \\mathbf {\\Phi } }{\\arg \\min }}\\sum _{i=1}^{n}{\\frac {1}{2}}{\\hat {h}}_{m}(x_{i})\\left[-{\\frac {{\\hat {g}}_{m}(x_{i})}{{\\hat {h}}_{m}(x_{i})}}-\\phi (x_{i})\\right]^{2}.}\n",
        "$$\n",
        "$$\n",
        "{\\displaystyle f_{m}(x)=\\alpha {\\hat {\\phi }}_{m}(x).}\n",
        "$$\n",
        " \n",
        " 3. Atualizar o modelo:\n",
        "$$\n",
        "{p^{(m)}={p}^{(m-1)}+{{f}}_{m}(x).}\n",
        "$$\n",
        "3. Output: $p^{(M)}.$\n",
        "\n",
        "\n",
        "<p align = \"justify\"> <b>Observação: </b> Existem implementações do XGoost para computação paralela e distribuída, as quais estão fora do escopo do trabalho."
      ],
      "metadata": {
        "id": "SJLqBvi6h_nU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prós e contras.\n",
        "\n",
        "Prós:\n",
        "*   Alta velocidade quando comparado com outros algorítmos.\n",
        "*   Grande desempenho para dados tabulares/estruturados.\n",
        "*   O termo de regularização do algorítmo penaliza a construção de árvores complexas, evitando o sobreajuste.\n",
        "*   Pode lidar com valores ausentes.\n",
        "*   Não há necessidade de dimensionar/normalizar dados.\n",
        "*   Admite implementação escalável, podendo ser usar computação paralela e distribuída.\n",
        "\n",
        "Contras:\n",
        "*   Não explora todas as estruturas de árvore possíveis.\n",
        "*   Pode deixar a desejar se os dados não forem estruturados (ex. imagens).\n",
        "*   Se os parâmetros da função objetivo não forem ajustados corretamente, pode facilmente sobreajustar os dados.\n"
      ],
      "metadata": {
        "id": "ldaaT4p-tVqD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparando o XGboost com outros algorítmos"
      ],
      "metadata": {
        "id": "3uM28NagXYbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "5oJ8v8qGZtsi"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "np.random.seed(seed)"
      ],
      "metadata": {
        "id": "q1QwGSXGr8nK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "def display_scores(scores):\n",
        "    print(\"Scores:\", scores)\n",
        "    print(\"Mean:\", scores.mean())\n",
        "    print(\"Standard deviation:\", scores.std())"
      ],
      "metadata": {
        "id": "YHNJY5TGXI4u"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regressão - Dataset California Housing\n",
        "\n",
        "O objetivo aqui é predizer valores de imóveis.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4BJOtRRQWqZ4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "gf4FmNkIDRoe"
      },
      "outputs": [],
      "source": [
        "url_ch1 = 'https://raw.githubusercontent.com/V4k0s/XGBoost/master/Datasets/california_housing_train.csv'\n",
        "\n",
        "housing = pd.read_csv(url_ch1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = housing.drop(\"median_house_value\", axis=1)\n",
        "Y = housing['median_house_value']\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, \n",
        "                                        test_size=0.2, \n",
        "                                        random_state=seed)"
      ],
      "metadata": {
        "id": "M5bUhiToZjDv"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "forest_reg = RandomForestRegressor(max_features=8, n_estimators=30, random_state=seed)\n",
        "forest_scores = cross_val_score(forest_reg, X_train, Y_train,\n",
        "                                scoring=\"neg_mean_squared_error\", cv=10)\n",
        "forest_rmse_scores = np.sqrt(-forest_scores)\n",
        "display_scores(forest_rmse_scores)"
      ],
      "metadata": {
        "id": "owrKme88Rvlv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb6b1d4d-76ef-4aba-bd3c-98603a4f61d9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scores: [53411.71987219 50601.07815024 47668.02059031 53267.30741694\n",
            " 47343.05818952 48416.20612185 53355.49522704 43516.46869185\n",
            " 51094.49428672 50324.30086461]\n",
            "Mean: 49899.814941125354\n",
            "Standard deviation: 3036.124845297608\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor\n",
        "xg_reg = XGBRegressor(learning_rate=0.08, \n",
        "                                 n_estimators=1300,\n",
        "                                 max_depth=6,\n",
        "                                 random_state=seed,\n",
        "                                 gpu_id='0',\n",
        "                                 booster=\"gbtree\",\n",
        "                                 warm_start = True,\n",
        "                                 objective='reg:squarederror'\n",
        "                                 ) \n",
        "\n",
        "xg_scores = cross_val_score(xg_reg, X_train, Y_train,\n",
        "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
        "xg_rmse_scores = np.sqrt(-xg_scores)\n",
        "display_scores(xg_rmse_scores)"
      ],
      "metadata": {
        "id": "idwUZmQQbHb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69d063a1-af4c-4554-d748-607373fe4554"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scores: [48083.02355567 45591.34735867 44351.16884563 48411.29139488\n",
            " 44264.62137323 43599.57509329 48203.12307105 42048.7401153\n",
            " 49217.61988536 47014.6680959 ]\n",
            "Mean: 46078.51787889657\n",
            "Standard deviation: 2315.1612823618702\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xg_reg.fit(X_train, Y_train)\n",
        "y_pred = xg_reg.predict(X_test)\n",
        "\n",
        "np.sqrt(MSE(Y_test, y_pred))"
      ],
      "metadata": {
        "id": "a3PY5z0B0rjP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "380f5c82-9304-40fc-cfa1-4ec9f0327b63"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "46653.07933960706"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Classificação - Dataset College\n",
        "\n",
        "O objetivo aqui é predizer se uma determinada escola é particular ou pública."
      ],
      "metadata": {
        "id": "hwPBIowicn3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url_cll = 'https://raw.githubusercontent.com/V4k0s/XGBoost/master/Datasets/college.csv'\n",
        "        \n",
        "college = pd.read_csv(url_cll)\n",
        "\n",
        "X = college.drop('private', axis=1)\n",
        "Y = college.private\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, \n",
        "                                        test_size=0.2, \n",
        "                                        random_state=seed)"
      ],
      "metadata": {
        "id": "XBARCB-rcn_H"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "trclf = DecisionTreeClassifier(random_state=seed)\n",
        "\n",
        "trclf_scores = cross_val_score(trclf, X_train, Y_train, cv=10, scoring='accuracy')\n",
        "print(np.mean(trclf_scores))"
      ],
      "metadata": {
        "id": "-4Z8GJKPSR2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f0d1cb7-3a7b-4260-d7d3-06a6a82fe2d1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8922171018945212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgclf = XGBClassifier(random_state=seed)\n",
        "\n",
        "xgclf_scores = cross_val_score(xgclf, X_train, Y_train, cv=10, scoring='accuracy')\n",
        "print(np.mean(xgclf_scores))"
      ],
      "metadata": {
        "id": "6cTc6TJVfm6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3fcf24b-a9a0-4111-c9ef-7496369a7e97"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9389144905273937\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xgclf.fit(X_train, Y_train)\n",
        "Y_pred = xgclf.predict(X_test)\n",
        "\n",
        "print(accuracy_score(Y_test,Y_pred))"
      ],
      "metadata": {
        "id": "YCPqjBgogZ7p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e501de9-24fd-4b10-ea9d-c95b05ec75c3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9294871794871795\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Classificação - Dataset Titanic\n",
        "\n",
        "O objetivo aqui é predizer se determinada pessoa vai sobreviver ao desastre ocorrido com o navio Titanic."
      ],
      "metadata": {
        "id": "mjjLzKJ-FZ39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url_tc1 = 'https://raw.githubusercontent.com/V4k0s/XGBoost/master/Datasets/titanic_train.csv'\n",
        "\n",
        "\n",
        "titanic = pd.read_csv(url_tc1)"
      ],
      "metadata": {
        "id": "b5exnFkjFZ3_"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Tratamento dos dados ##\n",
        "titanic.drop(['PassengerId','Name','Ticket','Cabin'],axis=1,inplace=True)\n",
        "titanic['Age'].fillna(titanic['Age'].mean(),inplace=True)\n",
        "titanic['Fare'] = titanic['Fare'].replace(0, titanic['Fare'].mean())\n",
        "titanic['Fare'].replace('nan',np.nan,inplace=True)\n",
        "titanic['Fare'].fillna(titanic['Fare'].mean(),inplace=True)\n",
        "titanic['Embarked'].replace('nan',np.nan,inplace=True)\n",
        "titanic['Embarked'].fillna(titanic['Embarked'].mode()[0],inplace=True)\n",
        "titanic['Sex']=titanic['Sex'].map({'male':0,'female':1})\n",
        "titanic['Embarked']=titanic['Embarked'].map({'S':0,'C':1,'Q':2})\n",
        "titanic['Age']=np.log(titanic['Age'])\n",
        "titanic['Fare']=np.log(titanic['Fare'])\n",
        "\n",
        "X = titanic.drop(['Survived'], axis=1)\n",
        "Y = titanic['Survived']\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, \n",
        "                                        test_size=0.3, \n",
        "                                        random_state=seed)"
      ],
      "metadata": {
        "id": "Q-Uc_78YoPwr"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "trclf = DecisionTreeClassifier(random_state=seed)\n",
        "\n",
        "trclf_scores = cross_val_score(trclf, X_train, Y_train, cv=10, scoring='accuracy')\n",
        "print(np.mean(trclf_scores))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19713af4-8051-484d-9b65-327d488d2cc4",
        "id": "TAJLsKzXFZ3_"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7626216077828981\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "xgclf = XGBClassifier(random_state=seed)\n",
        "xgclf_scores = cross_val_score(xgclf, X_train, Y_train, cv=10, scoring='accuracy')\n",
        "\n",
        "print(np.mean(xgclf_scores))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f380fd2c-8447-46d0-a20d-73bf3508c7e6",
        "id": "vaq5ync-FZ4A"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8282130056323604\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grid_param = {  \n",
        "    'n_estimators': [10, 50, 75, 115],\n",
        "    'max_depth': [3, 4, 5],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'early_stopping_rounds': [2, 3, 4, 5],\n",
        "    'warm_start': [True]}\n",
        "    \n",
        "g_search = GridSearchCV(estimator = xgclf, param_grid = grid_param, \n",
        "                     cv = 3, n_jobs = -1, verbose = 2)\n",
        "\n",
        "g_search.fit(X_train, Y_train)\n",
        "print(g_search.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpwSSHastzD6",
        "outputId": "9cb21aeb-1e96-43be-e9aa-6fcf345241fe"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n",
            "{'early_stopping_rounds': 2, 'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 75, 'warm_start': True}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g_search.score(X_test,Y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "728efdb2-4220-448e-a937-6f8b05fd1881",
        "id": "qtYZqzSPFZ4A"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8171641791044776"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    }
  ]
}